<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="viewport" content="width=device-width">
<link rel="stylesheet" href="/past/edh7916/2020/spring/assets/css/style.css">
<link rel="stylesheet" href="/past/edh7916/2020/spring/assets/css/syntax_default.css">
<link rel="shortcut icon" type="image/png" href="/past/edh7916/2020/spring/assets/img/favicon.ico">
<link crossorigin="anonymous" media="all" integrity="sha512-uhAd27cNiLn0VE2GVEVUN8D5zW0o7s0QTnCGMnJZkL2HqN9/LwHDi4ndTPJH0upUQHYl/8QF6cwbOYp/KIzlJQ==" rel="stylesheet" href="https://github.githubassets.com/assets/github-be4e45349cf088df7a6636f437c0a167.css" />
<script defer src="/past/edh7916/2020/spring/assets/js/all.min.js"></script>
<!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>
<script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>

<title>edquant | EDH 7916: Contemporary Research in Higher Education</title>

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>EDH 7916: Contemporary Research in Higher Education<a href="https://github.com/edquant/past" class="iconlink">
	      <i class="fab fa-github fa-sm"></i></a></h1>
	<h2 class="thin">Spring 2020</h2>
	<p>A course in quantitative research workflow for students in
the higher education administration program at the University of Florida
</p>
	<!-- side / top bar menu -->
	<h2 class="thin">
	  <a href="/past/edh7916/2020/spring/">Overview</a></br>
	  <a href="/past/edh7916/2020/spring/syllabus/">Course
	    information</a></br>
	  <a href="/past/edh7916/2020/spring/location/">Meeting
	    location</a></br>
	  <a href="/past/edh7916/2020/spring/start/">Getting started</a></br>
	  <a href="/past/edh7916/2020/spring/schedule/">Schedule</a></br>
	  <a href="/past/edh7916/2020/spring/lessons/">Lessons</a></br>
	  <a href="/past/edh7916/2020/spring/assignments/">Assignments</a></br>
	  <a href="/past/edh7916/2020/spring/questions/">Questions</a></br>
	  <a href="/past/edh7916/2020/spring/about/">About</a></br>	  
	</h2>
      </header>

      <section>
	<h1>Fitting regression models</h1>
	

	
<p>
  
  <a href="/past/edh7916/2020/spring/assets/pdf/modeling.pdf"
     class="iconlink" download title="Get PDF of lesson">
  <i class="far fa-file-pdf fa-2x"></i>
  </a>
  
  &nbsp;&nbsp;
  
  <a href="/past/edh7916/2020/spring/scripts/modeling.R"
     class="iconlink" download title="Get script">
    <i class="fas fa-code fa-2x"></i>
  </a>
  &nbsp;&nbsp;
  
  
  
  
  <a href="/past/edh7916/2020/spring/data/els_plans.dta"
     class="iconlink" download title="Get data">
    <i class="fas fa-database fa-2x"></i>
  </a>
  
</p>


<p>After your data have been wrangled from raw values to an analysis data
set and you’ve explored it with summary statistics and graphics, you are
ready to model it and begin making inferences. As one should expect from
a statistical language, R has a powerful system for fitting statistical
and econometric models.</p>

<p>In this lesson, we’ll use data from the <a href="https://nces.ed.gov/surveys/els2002/">NCES Education Longitudinal
Study of 2002</a>. Much like HSLS,
ELS is a nationally representative survey that initially surveyed
students in their early high school career (10th grade) and followed
them into college and the workforce. We’ll use a smaller version of it
because, unlike HSLS, the public use files for ELS include the variables
we need to properly account for the survey design when we weight our
data. Here’s a codebook with descriptions of the variables included in
our lesson today:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">variable</th>
      <th style="text-align: left">description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">stu_id</td>
      <td style="text-align: left">student id</td>
    </tr>
    <tr>
      <td style="text-align: left">sch_id</td>
      <td style="text-align: left">school id</td>
    </tr>
    <tr>
      <td style="text-align: left">strat_id</td>
      <td style="text-align: left">stratum</td>
    </tr>
    <tr>
      <td style="text-align: left">psu</td>
      <td style="text-align: left">primary sampling unit</td>
    </tr>
    <tr>
      <td style="text-align: left">bystuwt</td>
      <td style="text-align: left">student weight</td>
    </tr>
    <tr>
      <td style="text-align: left">bysex</td>
      <td style="text-align: left">sex-composite</td>
    </tr>
    <tr>
      <td style="text-align: left">byrace</td>
      <td style="text-align: left">student’s race/ethnicity-composite</td>
    </tr>
    <tr>
      <td style="text-align: left">bydob_p</td>
      <td style="text-align: left">student’s year and month of birth</td>
    </tr>
    <tr>
      <td style="text-align: left">bypared</td>
      <td style="text-align: left">parents’ highest level of education</td>
    </tr>
    <tr>
      <td style="text-align: left">bymothed</td>
      <td style="text-align: left">mother’s highest level of education-composite</td>
    </tr>
    <tr>
      <td style="text-align: left">byfathed</td>
      <td style="text-align: left">father’s highest level of education-composite</td>
    </tr>
    <tr>
      <td style="text-align: left">byincome</td>
      <td style="text-align: left">total family income from all sources 2001-composite</td>
    </tr>
    <tr>
      <td style="text-align: left">byses1</td>
      <td style="text-align: left">socio-economic status composite, v.1</td>
    </tr>
    <tr>
      <td style="text-align: left">byses2</td>
      <td style="text-align: left">socio-economic status composite, v.2</td>
    </tr>
    <tr>
      <td style="text-align: left">bystexp</td>
      <td style="text-align: left">how far in school student thinks will get-composite</td>
    </tr>
    <tr>
      <td style="text-align: left">bynels2m</td>
      <td style="text-align: left">els-nels 1992 scale equated sophomore math score</td>
    </tr>
    <tr>
      <td style="text-align: left">bynels2r</td>
      <td style="text-align: left">els-nels 1992 scale equated sophomore reading score</td>
    </tr>
    <tr>
      <td style="text-align: left">f1qwt</td>
      <td style="text-align: left">questionnaire weight for f1</td>
    </tr>
    <tr>
      <td style="text-align: left">f1pnlwt</td>
      <td style="text-align: left">panel weight, by and f1 (2002 and 2004)</td>
    </tr>
    <tr>
      <td style="text-align: left">f1psepln</td>
      <td style="text-align: left">f1 post-secondary plans right after high school</td>
    </tr>
    <tr>
      <td style="text-align: left">f2ps1sec</td>
      <td style="text-align: left">Sector of first postsecondary institution</td>
    </tr>
    <tr>
      <td style="text-align: left">female</td>
      <td style="text-align: left">== 1 if female</td>
    </tr>
    <tr>
      <td style="text-align: left">moth_ba</td>
      <td style="text-align: left">== 1 if mother has BA/BS</td>
    </tr>
    <tr>
      <td style="text-align: left">fath_ba</td>
      <td style="text-align: left">== 1 if father has BA/BS</td>
    </tr>
    <tr>
      <td style="text-align: left">par_ba</td>
      <td style="text-align: left">== 1 if either parent has BA/BS</td>
    </tr>
    <tr>
      <td style="text-align: left">plan_col_grad</td>
      <td style="text-align: left">== 1 if student plans to earn college degree</td>
    </tr>
    <tr>
      <td style="text-align: left">lowinc</td>
      <td style="text-align: left">== 1 if income &lt; $25k</td>
    </tr>
  </tbody>
</table>

<p>Let’s load the libraries and data!</p>

<p>…but first! You’ll most likely need to install the <a href="https://CRAN.R-project.org/package=survey"><code class="language-plaintext highlighter-rouge">survey</code>
package</a> first, using
<code class="language-plaintext highlighter-rouge">install.packages("survey")</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## libraries</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──

✔ ggplot2 3.3.0     ✔ purrr   0.3.4
✔ tibble  3.0.1     ✔ dplyr   0.8.5
✔ tidyr   1.0.2     ✔ stringr 1.4.0
✔ readr   1.3.1     ✔ forcats 0.5.0

── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">haven</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">survey</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in library(survey): there is no package called 'survey'
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## directory paths</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="c1">## assume we're running this script from the ./scripts subdirectory</span><span class="w">
</span><span class="n">dat_dir</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">file.path</span><span class="p">(</span><span class="s2">".."</span><span class="p">,</span><span class="w"> </span><span class="s2">"data"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## input data</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="c1">## assume we're running this script from the ./scripts subdirectory</span><span class="w">
</span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_dta</span><span class="p">(</span><span class="n">file.path</span><span class="p">(</span><span class="n">dat_dir</span><span class="p">,</span><span class="w"> </span><span class="s2">"els_plans.dta"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h1 id="t-test">t-test</h1>

<p>One common statistical test is a t-test for a difference in means across
groups (there are, of course, <a href="https://www.rdocumentation.org/packages/stats/versions/3.4.3/topics/t.test">others and R can compute
them</a>).
This version of the test can be computed using the R formula syntax: <code class="language-plaintext highlighter-rouge">y
~ x</code>. In our example, we’ll compute base-year math scores against
mother’s college education level. Notice that since we have the <code class="language-plaintext highlighter-rouge">data
= df</code> argument after the comma, we don’t need to include <code class="language-plaintext highlighter-rouge">df$</code> before
the two variables.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## t-test of difference in math scores across mother education (BA/BA or not)</span><span class="w">
</span><span class="n">t.test</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">moth_ba</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">var.equal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    Two Sample t-test

data:  bynels2m by moth_ba
t = -35.751, df = 15234, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -8.961638 -8.030040
sample estimates:
mean in group 0 mean in group 1 
       43.04709        51.54293 
</code></pre></div></div>

<blockquote>
  <h4 id="quick-exercise">Quick exercise</h4>

  <p>Run a t-test of reading scores against whether the father has a
Bachelor’s degree (<code class="language-plaintext highlighter-rouge">fath_ba</code>).</p>
</blockquote>

<h1 id="linear-model">Linear model</h1>

<p>Linear models are the go-to method of making inferences for many data
analysts. In R, the <code class="language-plaintext highlighter-rouge">lm()</code> command is used to compute an OLS regression.
Unlike above, where we just let the <code class="language-plaintext highlighter-rouge">t.test()</code> output print to the
console, we can and will store the output in an object.</p>

<p>First, let’s compute the same t-test but in a regression framework.
Because we assumed equal variances between the distributions in the
t-test above (<code class="language-plaintext highlighter-rouge">var.equal = TRUE</code>), we should get the same results as we
did before.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## compute same test as above, but in a linear model</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">moth_ba</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">fit</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ moth_ba, data = df)

Coefficients:
(Intercept)      moth_ba  
     43.047        8.496  
</code></pre></div></div>

<p>The output is a little thin: just the coefficients. To see the full
range of information you want from regression output, use the
<code class="language-plaintext highlighter-rouge">summary()</code> function wrapped around the <code class="language-plaintext highlighter-rouge">fit</code> object.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## use summary to see more information about regression</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ moth_ba, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-36.073  -9.869   0.593  10.004  36.223 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  43.0471     0.1245  345.84   &lt;2e-16 ***
moth_ba       8.4958     0.2376   35.75   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 13.09 on 15234 degrees of freedom
  (924 observations deleted due to missingness)
Multiple R-squared:  0.07741,   Adjusted R-squared:  0.07735 
F-statistic:  1278 on 1 and 15234 DF,  p-value: &lt; 2.2e-16
</code></pre></div></div>

<p>Looks like the coefficient on <code class="language-plaintext highlighter-rouge">moth_ed</code>, <code class="language-plaintext highlighter-rouge">8.4958392</code>, is the same as the
difference between the groups in the ttest, <code class="language-plaintext highlighter-rouge">8.4958392</code>, and the test
statistics are the same value: <code class="language-plaintext highlighter-rouge">-35.7511914</code>. Success!</p>

<h2 id="multiple-regression">Multiple regression</h2>

<p>To fit a multiple regression, use the same formula framework that we’ve
use before with the addition of all the terms you want on right-hand
side of the equation separated by plus (<code class="language-plaintext highlighter-rouge">+</code>) signs.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## linear model with more than one covariate on the RHS</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">byses1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">moth_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fath_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lowinc</span><span class="p">,</span><span class="w">
          </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + 
    lowinc, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.456  -8.775   0.432   9.110  40.921 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  45.7155     0.1811 252.420  &lt; 2e-16 ***
byses1        6.8058     0.2387  28.511  &lt; 2e-16 ***
female       -1.1483     0.1985  -5.784 7.42e-09 ***
moth_ba       0.4961     0.2892   1.715  0.08631 .  
fath_ba       0.8242     0.2903   2.840  0.00452 ** 
lowinc       -2.1425     0.2947  -7.271 3.75e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.24 on 15230 degrees of freedom
  (924 observations deleted due to missingness)
Multiple R-squared:  0.1929,    Adjusted R-squared:  0.1926 
F-statistic: 728.1 on 5 and 15230 DF,  p-value: &lt; 2.2e-16
</code></pre></div></div>

<p>The full output tells you:</p>

<ul>
  <li>the model that you fit, under <code class="language-plaintext highlighter-rouge">Call:</code></li>
  <li>a table of coefficients with
    <ul>
      <li>the main estimate (<code class="language-plaintext highlighter-rouge">Estimate</code>)</li>
      <li>the estimate error (<code class="language-plaintext highlighter-rouge">Std. Error</code>)</li>
      <li>the test statistic (<code class="language-plaintext highlighter-rouge">t value</code> with this model)</li>
      <li>the p value (<code class="language-plaintext highlighter-rouge">Pr(&gt;|t|</code>)</li>
    </ul>
  </li>
  <li>significance stars (<code class="language-plaintext highlighter-rouge">.</code> and <code class="language-plaintext highlighter-rouge">*</code>) along with legend</li>
  <li>the R-squared values (<code class="language-plaintext highlighter-rouge">Multiple R-squared</code> and <code class="language-plaintext highlighter-rouge">Adjusted R-squared</code>)</li>
  <li>the model F-statistic (<code class="language-plaintext highlighter-rouge">F-statistic</code>)</li>
  <li>number of observations dropped if any</li>
</ul>

<p>If observations were dropped, you can recover the number of observations
used with the <code class="language-plaintext highlighter-rouge">nobs()</code> function.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## check number of observations</span><span class="w">
</span><span class="n">nobs</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 15236
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">fit</code> object also holds a lot of other information that is sometimes
useful.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## see what fit object holds</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "na.action"     "xlevels"       "call"          "terms"        
[13] "model"        
</code></pre></div></div>

<p>For example, both <code class="language-plaintext highlighter-rouge">fitted.values</code> and <code class="language-plaintext highlighter-rouge">residuals</code> are stored in the
object. You can access these “hidden” attributes by treating the <code class="language-plaintext highlighter-rouge">fit</code>
object like a data frame and using the <code class="language-plaintext highlighter-rouge">$</code> notation.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## see first few fitted values and residuals</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">fit</span><span class="o">$</span><span class="n">fitted.values</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       1        2        3        4        5        6 
42.86583 48.51465 38.78234 36.98010 32.82855 38.43332 
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">fit</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>         1          2          3          4          5          6 
  4.974173   6.785347  27.457659  -1.650095  -2.858552 -14.153323 
</code></pre></div></div>

<blockquote>
  <h4 id="quick-exercise-1">Quick exercise</h4>

  <p>Add the fitted values to the residuals and store in an object (<code class="language-plaintext highlighter-rouge">x</code>).
Compare these values to the math scores in the data frame.</p>
</blockquote>

<p>As a final note, the model matrix used fit the regression can be
retrieved using <code class="language-plaintext highlighter-rouge">model.matrix()</code>. Since we have a lot of observations,
we’ll just look at the first few rows.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## see the design matrix</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">fit</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (Intercept) byses1 female moth_ba fath_ba lowinc
1           1  -0.25      1       0       0      0
2           1   0.58      1       0       0      0
3           1  -0.85      1       0       0      0
4           1  -0.80      1       0       0      1
5           1  -1.41      1       0       0      1
6           1  -1.07      0       0       0      0
</code></pre></div></div>

<p>What this shows is that the fit object actually stores a copy of the
data used to run it. That’s really convenient if you want to save the
object to disk (with the <code class="language-plaintext highlighter-rouge">save()</code> function) so you can review the
regression results later. But keep in mind that if you share that file,
you are sharing the part of the data used to estimate it.</p>

<h2 id="using-categorical-variables-or-factors">Using categorical variables or factors</h2>

<p>It’s not necessary to pre-construct dummy variables if you want to use a
categorical variable in your model. Instead you can use the categorical
variable wrapped in the <code class="language-plaintext highlighter-rouge">factor()</code> function. This tells R that the
underlying variable shouldn’t be treated as a continuous value, but
should be discrete groups. R will make the dummy variables on the fly
when fitting the model. We’ll include the categorical variable <code class="language-plaintext highlighter-rouge">byrace</code>
in this model.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## add factors</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">byses1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">moth_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fath_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lowinc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">byrace</span><span class="p">),</span><span class="w">
          </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + 
    lowinc + factor(byrace), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.154  -8.387   0.424   8.639  39.873 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      40.6995     1.0386  39.189  &lt; 2e-16 ***
byses1            5.7360     0.2345  24.456  &lt; 2e-16 ***
female           -1.1955     0.1900  -6.293 3.21e-10 ***
moth_ba           0.6500     0.2780   2.338 0.019407 *  
fath_ba           0.8482     0.2796   3.033 0.002423 ** 
lowinc           -1.2536     0.2839  -4.416 1.01e-05 ***
factor(byrace)2   9.1538     1.0740   8.523  &lt; 2e-16 ***
factor(byrace)3  -2.2185     1.0603  -2.092 0.036419 *  
factor(byrace)4   1.2778     1.0937   1.168 0.242696    
factor(byrace)5   0.2387     1.0814   0.221 0.825295    
factor(byrace)6   4.2457     1.1158   3.805 0.000142 ***
factor(byrace)7   6.9514     1.0379   6.698 2.19e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.71 on 15224 degrees of freedom
  (924 observations deleted due to missingness)
Multiple R-squared:  0.2613,    Adjusted R-squared:  0.2608 
F-statistic: 489.6 on 11 and 15224 DF,  p-value: &lt; 2.2e-16
</code></pre></div></div>

<p>If you’re using labeled data like we have been for the past couple of
modules, you can use the <code class="language-plaintext highlighter-rouge">as_factor()</code> function from the <a href="https://www.rdocumentation.org/packages/haven/versions/1.1.0/topics/as_factor">haven
library</a>
in place of the base <code class="language-plaintext highlighter-rouge">factor()</code> function. You’ll still see the
<code class="language-plaintext highlighter-rouge">as_factor(&lt;var&gt;)</code> prefix on each coefficient, but now you’ll have
labels instead of the underlying values, which should make parsing the
output a little easier.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## same model, but use as_factor() instead of factor() to use labels</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">byses1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">moth_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fath_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lowinc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">as_factor</span><span class="p">(</span><span class="n">byrace</span><span class="p">),</span><span class="w">
          </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + 
    lowinc + as_factor(byrace), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-39.154  -8.387   0.424   8.639  39.873 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                 40.6995     1.0386  39.189  &lt; 2e-16 ***
byses1                       5.7360     0.2345  24.456  &lt; 2e-16 ***
female                      -1.1955     0.1900  -6.293 3.21e-10 ***
moth_ba                      0.6500     0.2780   2.338 0.019407 *  
fath_ba                      0.8482     0.2796   3.033 0.002423 ** 
lowinc                      -1.2536     0.2839  -4.416 1.01e-05 ***
as_factor(byrace)asian_pi    9.1538     1.0740   8.523  &lt; 2e-16 ***
as_factor(byrace)black_aa   -2.2185     1.0603  -2.092 0.036419 *  
as_factor(byrace)hisp_nr     1.2778     1.0937   1.168 0.242696    
as_factor(byrace)hisp_rs     0.2387     1.0814   0.221 0.825295    
as_factor(byrace)mult_race   4.2457     1.1158   3.805 0.000142 ***
as_factor(byrace)white       6.9514     1.0379   6.698 2.19e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.71 on 15224 degrees of freedom
  (924 observations deleted due to missingness)
Multiple R-squared:  0.2613,    Adjusted R-squared:  0.2608 
F-statistic: 489.6 on 11 and 15224 DF,  p-value: &lt; 2.2e-16
</code></pre></div></div>

<p>If you look at the model matrix, you can see how R created the dummy
variables from <code class="language-plaintext highlighter-rouge">byrace</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## see what R did under the hood to convert categorical to dummies</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">fit</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (Intercept) byses1 female moth_ba fath_ba lowinc as_factor(byrace)asian_pi
1           1  -0.25      1       0       0      0                         0
2           1   0.58      1       0       0      0                         1
3           1  -0.85      1       0       0      0                         0
4           1  -0.80      1       0       0      1                         0
5           1  -1.41      1       0       0      1                         0
6           1  -1.07      0       0       0      0                         0
  as_factor(byrace)black_aa as_factor(byrace)hisp_nr as_factor(byrace)hisp_rs
1                         0                        0                        1
2                         0                        0                        0
3                         0                        0                        0
4                         1                        0                        0
5                         0                        1                        0
6                         0                        1                        0
  as_factor(byrace)mult_race as_factor(byrace)white
1                          0                      0
2                          0                      0
3                          0                      1
4                          0                      0
5                          0                      0
6                          0                      0
</code></pre></div></div>

<blockquote>
  <h4 id="quick-exercise-2">Quick exercise</h4>

  <p>Add the categorical variable <code class="language-plaintext highlighter-rouge">byincome</code> to the model above. Next use
<code class="language-plaintext highlighter-rouge">model.matrix()</code> to check the RHS matrix.</p>
</blockquote>

<h2 id="interactions">Interactions</h2>

<p>Add interactions to a regression using an asterisks (<code class="language-plaintext highlighter-rouge">*</code>) between the
terms you want to interact. This will add both main terms and the
interaction(s) between the two to the model. Any interaction terms will
be labeled using the base name or factor name of each term joined by a
colon (<code class="language-plaintext highlighter-rouge">:</code>).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## add interactions</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">byses1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">bypared</span><span class="p">)</span><span class="o">*</span><span class="n">lowinc</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ byses1 + factor(bypared) * lowinc, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-38.998  -8.852   0.326   9.063  39.257 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              44.0084     0.6345  69.355  &lt; 2e-16 ***
byses1                    7.6082     0.2772  27.448  &lt; 2e-16 ***
factor(bypared)2          1.5546     0.6544   2.376 0.017533 *  
factor(bypared)3          0.6534     0.7136   0.916 0.359863    
factor(bypared)4          1.8902     0.7198   2.626 0.008646 ** 
factor(bypared)5          1.5059     0.7200   2.091 0.036501 *  
factor(bypared)6          1.4527     0.7386   1.967 0.049235 *  
factor(bypared)7          2.0044     0.8286   2.419 0.015569 *  
factor(bypared)8          0.8190     0.9239   0.887 0.375360    
lowinc                    2.0347     0.8112   2.508 0.012140 *  
factor(bypared)2:lowinc  -2.9955     0.9298  -3.222 0.001278 ** 
factor(bypared)3:lowinc  -4.0551     1.0682  -3.796 0.000147 ***
factor(bypared)4:lowinc  -4.8143     1.1126  -4.327 1.52e-05 ***
factor(bypared)5:lowinc  -4.6890     1.0947  -4.283 1.85e-05 ***
factor(bypared)6:lowinc  -4.5252     1.0556  -4.287 1.82e-05 ***
factor(bypared)7:lowinc  -7.2222     1.3796  -5.235 1.67e-07 ***
factor(bypared)8:lowinc  -9.8773     1.6110  -6.131 8.94e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.23 on 15219 degrees of freedom
  (924 observations deleted due to missingness)
Multiple R-squared:  0.1948,    Adjusted R-squared:  0.194 
F-statistic: 230.2 on 16 and 15219 DF,  p-value: &lt; 2.2e-16
</code></pre></div></div>

<h2 id="polynomials">Polynomials</h2>

<p>To add quadratic and other polynomial terms to the model, use the <code class="language-plaintext highlighter-rouge">I()</code>
function, which lets you raise the term to the power you want in the
regression using the caret (<code class="language-plaintext highlighter-rouge">^</code>) operator. In the model below, we add a
quadratic version of the reading score to the right-hand side.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## add polynomials</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">bynels2r</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">I</span><span class="p">(</span><span class="n">bynels2r</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = bynels2m ~ bynels2r + I(bynels2r^2), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-33.462  -5.947  -0.156   5.780  46.645 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   12.7765815  0.6241194  20.471   &lt;2e-16 ***
bynels2r       1.1197116  0.0447500  25.021   &lt;2e-16 ***
I(bynels2r^2) -0.0006246  0.0007539  -0.828    0.407    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.921 on 15881 degrees of freedom
  (276 observations deleted due to missingness)
Multiple R-squared:  0.5658,    Adjusted R-squared:  0.5657 
F-statistic: 1.035e+04 on 2 and 15881 DF,  p-value: &lt; 2.2e-16
</code></pre></div></div>

<blockquote>
  <h4 id="quick-exercise-3">Quick exercise</h4>

  <p>Fit a linear model with both interactions and a polynomial term. Then
look at the model matrix to see what R did under the hood.</p>
</blockquote>

<h1 id="generalized-linear-model">Generalized linear model</h1>

<p>To fit a model with binary outcomes, switch to the <code class="language-plaintext highlighter-rouge">glm()</code> function. It
is set up just like <code class="language-plaintext highlighter-rouge">lm()</code>, but it has an extra argument, <code class="language-plaintext highlighter-rouge">family</code>. Set
the argument to <code class="language-plaintext highlighter-rouge">binomial()</code> when your dependent variable is binary. By
default, the <code class="language-plaintext highlighter-rouge">link</code> function is a
<a href="https://en.wikipedia.org/wiki/Logit">logit</a> link.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## logit</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">plan_col_grad</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">bynels2m</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">as_factor</span><span class="p">(</span><span class="n">bypared</span><span class="p">),</span><span class="w">
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w">
           </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binomial</span><span class="p">())</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
glm(formula = plan_col_grad ~ bynels2m + as_factor(bypared), 
    family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6467  -0.9581   0.5211   0.7695   1.5815  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.824413   0.090000 -20.271  &lt; 2e-16 ***
bynels2m                   0.056427   0.001636  34.491  &lt; 2e-16 ***
as_factor(bypared)hsged    0.042315   0.079973   0.529   0.5967    
as_factor(bypared)att2yr   0.204831   0.088837   2.306   0.0211 *  
as_factor(bypared)grad2ry  0.480828   0.092110   5.220 1.79e-07 ***
as_factor(bypared)att4yr   0.499019   0.090558   5.511 3.58e-08 ***
as_factor(bypared)grad4yr  0.754817   0.084271   8.957  &lt; 2e-16 ***
as_factor(bypared)ma       0.943558   0.101585   9.288  &lt; 2e-16 ***
as_factor(bypared)phprof   1.052006   0.121849   8.634  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 17545  on 15235  degrees of freedom
Residual deviance: 15371  on 15227  degrees of freedom
  (924 observations deleted due to missingness)
AIC: 15389

Number of Fisher Scoring iterations: 4
</code></pre></div></div>

<p>If you want a <a href="https://en.wikipedia.org/wiki/Probit_model">probit</a>
model, just change the link to <code class="language-plaintext highlighter-rouge">probit</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## probit</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">plan_col_grad</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">bynels2m</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">as_factor</span><span class="p">(</span><span class="n">bypared</span><span class="p">),</span><span class="w">
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w">
           </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binomial</span><span class="p">(</span><span class="n">link</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"probit"</span><span class="p">))</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
glm(formula = plan_col_grad ~ bynels2m + as_factor(bypared), 
    family = binomial(link = "probit"), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7665  -0.9796   0.5238   0.7812   1.5517  

Coefficients:
                            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.0522131  0.0539072 -19.519  &lt; 2e-16 ***
bynels2m                   0.0326902  0.0009357  34.938  &lt; 2e-16 ***
as_factor(bypared)hsged    0.0325415  0.0488225   0.667   0.5051    
as_factor(bypared)att2yr   0.1316456  0.0539301   2.441   0.0146 *  
as_factor(bypared)grad2ry  0.2958810  0.0554114   5.340 9.31e-08 ***
as_factor(bypared)att4yr   0.3065176  0.0544813   5.626 1.84e-08 ***
as_factor(bypared)grad4yr  0.4553127  0.0505009   9.016  &lt; 2e-16 ***
as_factor(bypared)ma       0.5525198  0.0588352   9.391  &lt; 2e-16 ***
as_factor(bypared)phprof   0.6115358  0.0688820   8.878  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 17545  on 15235  degrees of freedom
Residual deviance: 15379  on 15227  degrees of freedom
  (924 observations deleted due to missingness)
AIC: 15397

Number of Fisher Scoring iterations: 4
</code></pre></div></div>

<blockquote>
  <h4 id="quick-exercise-4">Quick exercise</h4>

  <p>Fit a logit or probit model to another binary outcome.</p>
</blockquote>

<h1 id="using-survey-weights">Using survey weights</h1>

<p>So far we haven’t used survey weights, but they are very important when
using survey data. To use survey weights, you’ll need to use the survey
package (which we’ve already loaded above)</p>

<p>To use survey weights, you need to set the survey design using the
<code class="language-plaintext highlighter-rouge">svydesign()</code> function. You could do this in the <code class="language-plaintext highlighter-rouge">svyglm()</code> function
we’ll use to actually estimate the equation, but it’s easier and
clearer to do it first, store it in an object, and then use that object
in the <code class="language-plaintext highlighter-rouge">syvglm()</code>.</p>

<p>ELS has a complex sampling design that we won’t get into, but the
appropriate columns from our data frame, <code class="language-plaintext highlighter-rouge">df</code>, are set to the proper
arguments in <code class="language-plaintext highlighter-rouge">svydesign()</code>:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ids</code> are the primary sampling units or <code class="language-plaintext highlighter-rouge">psu</code>s</li>
  <li><code class="language-plaintext highlighter-rouge">strata</code> are indicated by the <code class="language-plaintext highlighter-rouge">strat_id</code>s</li>
  <li><code class="language-plaintext highlighter-rouge">weight</code> is the base-year student weight or <code class="language-plaintext highlighter-rouge">bystuwt</code></li>
  <li><code class="language-plaintext highlighter-rouge">data</code> is our data frame object, <code class="language-plaintext highlighter-rouge">df</code></li>
  <li><code class="language-plaintext highlighter-rouge">nest = TRUE</code> because the <code class="language-plaintext highlighter-rouge">psu</code>s are nested in <code class="language-plaintext highlighter-rouge">strat_id</code>s</li>
</ul>

<p>Finally, notice the <code class="language-plaintext highlighter-rouge">~</code> before each column name, which is necessary in
this function.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## set svy design data</span><span class="w">
</span><span class="n">svy_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">svydesign</span><span class="p">(</span><span class="n">ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">psu</span><span class="p">,</span><span class="w">
                    </span><span class="n">strata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">strat_id</span><span class="p">,</span><span class="w">
                    </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">bystuwt</span><span class="p">,</span><span class="w">
                    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w">
                    </span><span class="n">nest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in svydesign(ids = ~psu, strata = ~strat_id, weight = ~bystuwt, : could not find function "svydesign"
</code></pre></div></div>

<p>Now that we’ve set the survey design, let’s compare the unweighted mean
with one that accounts for the survey design;</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## compare unweighted and survey-weighted mean of math scores</span><span class="w">
</span><span class="n">mean</span><span class="p">(</span><span class="n">df</span><span class="o">$</span><span class="n">bynels2m</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 45.35452
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">svymean</span><span class="p">(</span><span class="o">~</span><span class="n">bynels2m</span><span class="p">,</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">svy_df</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in svymean(~bynels2m, design = svy_df, na.rm = TRUE): could not find function "svymean"
</code></pre></div></div>

<p>We can also use the survey package to properly weight our t-tests and
regression models — again, using the object <code class="language-plaintext highlighter-rouge">svy_df</code> in the <code class="language-plaintext highlighter-rouge">design</code>
argument in place of our unset <code class="language-plaintext highlighter-rouge">df</code> data frame.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## get svymeans by group</span><span class="w">
</span><span class="n">svyby</span><span class="p">(</span><span class="o">~</span><span class="n">bynels2m</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">moth_ba</span><span class="p">,</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">svy_df</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">svymean</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in svyby(~bynels2m, by = ~moth_ba, design = svy_df, FUN = svymean, : could not find function "svyby"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## t-test using survey design / weights</span><span class="w">
</span><span class="n">svyttest</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">moth_ba</span><span class="p">,</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">svy_df</span><span class="p">,</span><span class="w"> </span><span class="n">var.equal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in svyttest(bynels2m ~ moth_ba, design = svy_df, var.equal = TRUE): could not find function "svyttest"
</code></pre></div></div>

<blockquote>
  <h4 id="quick-exercise-5">QUICK EXERCISE</h4>

  <p>Compare this to the output from <code class="language-plaintext highlighter-rouge">t.test()</code> above.</p>
</blockquote>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## fit the svyglm regression and show output</span><span class="w">
</span><span class="n">svyfit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">svyglm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">byses1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">moth_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fath_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lowinc</span><span class="p">,</span><span class="w">
                 </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">svy_df</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in svyglm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc, : could not find function "svyglm"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">svyfit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in summary(svyfit): object 'svyfit' not found
</code></pre></div></div>

<p>Note that the survey library has a ton of features and is worth diving
into if you regularly work with survey data.</p>

<h2 id="predictions">Predictions</h2>

<p>Being able to generate predictions from new data can be a powerful tool.
Above, we were able to return the predicted values from the fit object.
We can also use the <code class="language-plaintext highlighter-rouge">predict()</code> function to return the standard error of
the prediction in addition to the predicted values for new observations.</p>

<p>First, we’ll get predicted values using the original data along with
their standard errors.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## predict from first model</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">bynels2m</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">byses1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">moth_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fath_ba</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lowinc</span><span class="p">,</span><span class="w">
          </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">

</span><span class="c1">## old data</span><span class="w">
</span><span class="n">fit_pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">se.fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1">## show options</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">fit_pred</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] "fit"            "se.fit"         "df"             "residual.scale"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">fit_pred</span><span class="o">$</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       1        2        3        4        5        6 
42.86583 48.51465 38.78234 36.98010 32.82855 38.43332 
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">fit_pred</span><span class="o">$</span><span class="n">se.fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.1755431 0.2587681 0.2314676 0.2396327 0.2737971 0.2721818
</code></pre></div></div>

<h3 id="predictions-with-new-data">Predictions with new data</h3>

<p>Ideally, we would have a new observations with which to make
predictions. Then we could test our modeling choices by seeing how well
they predicted the outcomes of the new observations.</p>

<p>With discrete outcomes (like binary 0/1 data), for example, we could use
our model and right-hand side variables from new observations to predict
whether the new observation should have a 0 or 1 outcome. Then we could
compare those predictions to the actual observed outcomes by making a 2
by 2 <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>
that counted the numbers of true positives and negatives (correct
predictions) and false positives and negatives (incorrect predictions).</p>

<p>With continuous outcomes, we could follow the same procedure as above,
but rather than using a confusion matrix, instead assess our model
performance by measuring the error between our predictions and the
observed outcomes. Depending on our problem and model, we might care
about minimizing the root mean square error, the mean absolute error, or
some other metric of the error.</p>

<h3 id="predictions-using-training-and-testing-data">Predictions using training and testing data</h3>

<p>In the absence of new data, we instead could have separated our data
into two data sets, a <a href="https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets">training set and test
set</a>.
After fitting our model to the training data, we could have tested it by
following either above procedure with the testing data (depending on the
outcome type). Setting a rule for ourselves, we could evaluate how well
we did, that is, how well our training data model classified test data
outcomes, and perhaps decide to adjust our modeling assumptions.</p>

<h2 id="margins">Margins</h2>

<p>Using the <code class="language-plaintext highlighter-rouge">predict()</code> function alongside some other skills we have
practiced, we can also make predictions on the margin a la Stata’s
<a href="https://www.stata.com/help.cgi?margins"><code class="language-plaintext highlighter-rouge">-margins-</code> suite of commands</a>.</p>

<p>For example, after fitting our multiple regression, we might ask
ourselves, what is the marginal “effect” of having a low family income
on math scores, holding all other terms in our model constant?</p>

<p>To answer this question, we first need to make a “new” data frame with a
column each for the variables used in the model and rows that equal the
number of predictive margins that we want to create. In our example,
that means making a data frame with two rows and five columns.</p>

<p>With <code class="language-plaintext highlighter-rouge">lowinc</code>, the variable that we want to make marginal predictions
for, we have two potential values: 0 and 1. This is the reason our “new”
data frame has two rows. If <code class="language-plaintext highlighter-rouge">lowinc</code> took on four values, for example,
then our “new” data frame would have four rows, one for each potential
value. But since we have two, <code class="language-plaintext highlighter-rouge">lowinc</code> in our “new” data frame will
equal <code class="language-plaintext highlighter-rouge">0</code> in one row and <code class="language-plaintext highlighter-rouge">1</code> in the other row.</p>

<p>All other columns in the “new” data frame should have consistent values
down their rows. Often, each column’s repeated value is the variable’s
average in the data. Though we could use the original data frame (<code class="language-plaintext highlighter-rouge">df</code>)
to generate these averages, the resulting values may summarize different
data from what was used to fit the model if there were observations that
<code class="language-plaintext highlighter-rouge">lm()</code> dropped due to missing values. That happened with our model. We
could try to use the original data frame and account for dropped
observations, but I think it’s easier to use the design matrix that’s
retrieved from <code class="language-plaintext highlighter-rouge">model.matrix()</code>.</p>

<p>The code below goes step-by-step to make the “new” data frame.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## create new data that has two rows, with averages and one marginal change</span><span class="w">

</span><span class="c1">## (1) save model matrix</span><span class="w">
</span><span class="n">mm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">mm</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (Intercept) byses1 female moth_ba fath_ba lowinc
1           1  -0.25      1       0       0      0
2           1   0.58      1       0       0      0
3           1  -0.85      1       0       0      0
4           1  -0.80      1       0       0      1
5           1  -1.41      1       0       0      1
6           1  -1.07      0       0       0      0
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## (2) drop intercept column of ones (predict() doesn't need them)</span><span class="w">
</span><span class="n">mm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mm</span><span class="p">[,</span><span class="m">-1</span><span class="p">]</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">mm</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  byses1 female moth_ba fath_ba lowinc
1  -0.25      1       0       0      0
2   0.58      1       0       0      0
3  -0.85      1       0       0      0
4  -0.80      1       0       0      1
5  -1.41      1       0       0      1
6  -1.07      0       0       0      0
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## (3) convert to data frame so we can use $ notation in next step</span><span class="w">
</span><span class="n">mm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">mm</span><span class="p">)</span><span class="w">

</span><span class="c1">## (4) new data frame of means where only lowinc changes</span><span class="w">
</span><span class="n">new_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">byses1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">mm</span><span class="o">$</span><span class="n">byses1</span><span class="p">),</span><span class="w">
                     </span><span class="n">female</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">mm</span><span class="o">$</span><span class="n">female</span><span class="p">),</span><span class="w">
                     </span><span class="n">moth_ba</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">mm</span><span class="o">$</span><span class="n">moth_ba</span><span class="p">),</span><span class="w">
                     </span><span class="n">fath_ba</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">mm</span><span class="o">$</span><span class="n">fath_ba</span><span class="p">),</span><span class="w">
                     </span><span class="n">lowinc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w">

</span><span class="c1">## see new data</span><span class="w">
</span><span class="n">new_df</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      byses1    female   moth_ba   fath_ba lowinc
1 0.04210423 0.5027566 0.2743502 0.3195064      0
2 0.04210423 0.5027566 0.2743502 0.3195064      1
</code></pre></div></div>

<p>Notice how the new data frame has the same terms that were used in the
original model, but has only two rows. In the <code class="language-plaintext highlighter-rouge">lowinc</code> column, the
values switch from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">1</code>. All the other rows are averages of the
data used to fit the model.</p>

<p>To generate the prediction, we use the same function call as before, but
use our <code class="language-plaintext highlighter-rouge">new_df</code> object with the <code class="language-plaintext highlighter-rouge">newdata</code> argument.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## predict margins</span><span class="w">
</span><span class="n">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_df</span><span class="p">,</span><span class="w"> </span><span class="n">se.fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$fit
       1        2 
45.82426 43.68173 

$se.fit
        1         2 
0.1166453 0.2535000 

$df
[1] 15230

$residual.scale
[1] 12.24278
</code></pre></div></div>

<p>Our results show that compared to otherwise similar students, those with
a family income less than $25,000 a year are predicted to score about
two points lower on their math test.</p>

<p>In this example, we held the other covariates at their means. We could
have chosen other values (<em>e.g.</em> <code class="language-plaintext highlighter-rouge">fath_ba == 1</code> or <code class="language-plaintext highlighter-rouge">female == 1</code>),
however, meaning that we could use the same procedure to produce
predictions for low-income status (or other model covariates) across a
range of margins.</p>



	<!-- <p> -->
	<!--   Updated:  -->
	<!-- </p> -->
      </section>
      <footer>
        <p>
  Benjamin Skinner</br>  
  Assistant Professor</br>  
  University of Florida</br>
  <a href="https://www.btskinner.io" class="iconlink" alt="Personal website">
    <i class="fas fa-home fa-lg"></i></a> | 
  <a href="https://github.com/btskinner" class="iconlink" alt="Github
								profile">
    <i class="fab fa-github fa-lg"></i></a> |
  <a href="https://twitter.com/btskinner" class="iconlink" alt="Twitter profile">
    <i class="fab fa-twitter fa-lg"></i></a>
</p>
<p>
  <small>
    <a href="https://pages.github.com">GitHub Pages</a> |
    <a href="https://github.com/orderedlist/minimal">Theme</a> |
    <a href="/past/edh7916/2020/spring/releases/">Releases</a>
  </small>
</p>

      </footer>
    </div>

    <!-- load Javascript if page requires it -->
    
    
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-63981025-1', 'auto');
        ga('send', 'pageview');
    </script>
  
  </body>
</html>
